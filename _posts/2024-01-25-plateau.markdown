---
layout: post
title:  "The AI Plateau"
categories: jekyll update, AI
---
Bit of a backstory, past year might have been full of AI models left right and center, but this year feels different. Is GPT-4 the limit? Are other models even that good? What's the deal with Gemini and big tech cuffing people to work for them for such high risk and reward? And, biggest of them all, is AGI even achievable at this point, or it has been and will be an mirage all the way ahead?

### My timeline of AI events (very rough)

GPT took the world by storm early 2023, sparked and paved the way for more AI models fine tuned to do one thing or the other. Several competitors did rise in conjuction to OpenAI's remarkable models, yet none seemed to do that good of a job. Text is all good, but multimodal models were soon being seen to be in the market, just after individual image generation and video models peaked. Big tech companies went all in on the "AI" hype, incorporating it into each and every last bit of their products that could need it. Why? Because AI was the cool kid everyone wanted to interact with. Well, these huge "transformers" or so called "models" which were nothing but a huge sea of ML nets under the hood, needed huge compute to work. Hence, as models were on the move, compute needed to move as fast as that too, hence GPU's becoming the key to exactly that. NIVIDA did the job pretty damn well, selling shovels while everyone was out digging gold. Fast-forward to late 2023, we get pretty damn major updates to the existing models, and a few new models finally entered the scene. Multimodal models were a reality now, and sheesh are they good, made us wonder that the end is almost there.

Now, with all this software and all, did hardware stay the same? Glad you asked, cause no, hardware also had its rightful share on the AI clout. Multiple new age tech products were announced, which all would pave the way for a very hyper-realistic future. Once they become accessible to the general public, it will surely be like that futuristic scene from a sci-fi movie to say the less. Even vehicles had their own share, configuring interiors as they like and switching all electric. The whole world landscape changed, but on the large scale. AI startups, new extremely priced AI products, premium AI chatbots, home assistants, what not; but inly accessible to those at the top.

Job markets continued to recede, with layoffs hitting half of the workforce off their workdesks, and new grads finding it difficult to land their first web dev job. For sure, everyone was scared and on the edge of their seats hoping that they'd wake up to see another day at the office, everyday. Well, COVID was a factor back then too, remote jobs still tend to prevail. Some people believed that the next age of software engineers would just be the english majors, prompting their way in and out based off on the product managers demands which inturn were just the reflection of what the customers wanted. To start things off, that might happen and next thing we know, an all batteries included "prompting" framework governs the whole SDLC, while the tech lead, leads his team out of the process. But but but, some people beg to differ. It's until you're not proficient enough that you're proficient enough to question your skills and abilities. Code as we know it, needs a human to get human needs into them, an LLM can only do too much. Still, these people will ultimately admit that using a code assistant such as the "Copilot" gets work done much faster and easier. Also, companies paying people their costly cut, would just pefer to turn to a lifeless algorithm monster, who doesn't even understand emotionally the semantics of a pay raise, and does not have "human" inadequacies to it.

In case you're wondering:
_That's not what a timeline is? :/_ Well, idk I wrote what came to mind. 

### Alright, to the point now

Fast-forwarding to current year, the AI hype that shook the world away, seems to be calm at the point, at a pace which it just seems like it's plateauing or smh? AI is not big a deal now, cause everything around us conformed according to the lastest AI trend in the market; and it will be like this for a while now, I suppose. Everyone's living/coping with the fact that AGI might be just take over the world someday, given if we reach there. I'll be completely honest, being a person who's oblivious to most thing under the hood, I thought AGI will hit very very soon. But, it is not that simple, which I realized just primitively entering 2024. You see, no big model updates, no major breakthroughs and no more progress on AGI, just that plain old ChatGPT with the latest model ofc. Could be the fact that I expected much more drama coming into 2024, which certainly is not there yet. 

![Headline of the TLDR newsletter](/assets/media/need_more_compute!!!.png "Basically short on compute -Sam Altman, CEO OpenAI")

And when I see a headline like the one above at the top of my monday (all-things-tech) newsletter, it just makes me wonder:

> If we need more compute then there already is to make the already existing models better, shouldn't we question if this is the right way to make such models, aren't we doing something wrong here? About time a new solution to this problem should prevail over the already existing ones.

Now, I'm well aware that better models need better and more data to be better at what they do, but what I'm implying is should we be aware of the limit to all of this, as is the case with anything else?

All we can hope for is, put in more compute and someday the math will check itself out.

_P.S. This could be absolute BS cause I do not know of the depth of stuff._